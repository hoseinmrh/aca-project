# -*- coding: utf-8 -*-
"""Classifying Proteins: Comparing 5 GNN Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/scratchpad/classifying-proteins-comparing-5-gnn-models.355993bb-9802-4f70-9fa2-12370f82830f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250519/auto/storage/goog4_request%26X-Goog-Date%3D20250519T144544Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D37e62a7e03f53dffc24fee3925728a4f94ff9038209d8b7b3c43bf6a2a0d7d034abe4213233ff88c65d9302c4d4b6143b50325d06f6597407447670795964c7abe5bdb203bea37ccc125fca18b8c52a2b1ee7697529f4af51f143bf91e5eface3f243d027eda360b5700558ed43c7ae2f0103d079c6872a207fbc7fdb075ed45de7ad67bcea71620ef48629e33671c6ec5d42b71b0121b815ee5cfef5ed1135a147581ab47fb6e2fcac07b15d20279a0588bd5986dbcde3acd74d1608c1ec0f56588dbb35a675f0a9698bba78a95c1908c9f0594d9137df87d49f1e1aaef03924efca5420866dbff063a8866e6c36427af4c951e185c0d453e2d6bb770844744

In recent years, the field of graph modeling has experienced a remarkable surge in interest. This surge is largely fueled by the recognition that many real-world phenomena can be effectively represented as networks or graphs. These graphs capture intricate relationships between various entities within a given system, spanning from people and objects to molecules and even brain signals.

This realization has sparked a burgeoning interest in different graph theories aimed at comprehensively exploring these graphical representations. Within the realm of machine learning and artificial intelligence, one framework has emerged as a particularly powerful tool for tackling these challenges: Graph Neural Networks (GNNs).

GNNs offer a versatile approach to modeling and analyzing graph-structured data, making them well-suited for a wide range of applications across various domains. As we delve deeper into the capabilities and applications of GNNs, it becomes increasingly evident that they hold immense potential for unraveling complex relationships and patterns hidden within graph data.

In this notebook, we delve into the capabilities of Graph Neural Networks (GNNs) in the classification of proteins as enzymes or non-enzymes. Proteins are made up of biomolecules and can be represented as graphs. We'll treat the amino acids in the proteins as entities (nodes) connected by pair-wise relationships (edges), creating a network or a graph.

We'll try out five different GNN models and compare how well they perform. By comparing their performances, we aim to gain insights into the effectiveness of these models in capturing the underlying structural characteristics of proteins and discerning enzymatic properties.

# Dataset to Use

PROTEINS is a common dataset within bioinformatics. Comprising **1113** individual graphs representing various proteins, each graph's nodes represent amino acids. A connection (relationship) between two nodes arises if their proximity falls below **0.6 nanometers**.

This dataset is sourced from TUDataset and can be implemented within PyTorch Geometric.

# GNN Models to Use

The 5 GNN models that will be implemented in this notebook are:
* Graph Convolutional Network (GCN)
* Graph Isomorphism Network (GIN)
* Graph Attention Network (GAT)
* ChebNet
* ChebNet augmented with EdgeConv

# Analysis and Implementations
First we install and load necessary libraries
"""

import os
import numpy as np
import pandas as pd
from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score
from copy import deepcopy
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch_geometric.nn import GCNConv, GATv2Conv, GINConv, ChebConv, EdgeConv, global_mean_pool, global_max_pool, global_add_pool
from torch.nn import Sequential, Linear, BatchNorm1d, ReLU
from torch_geometric.nn.inits import glorot, zeros
from collections import OrderedDict
from torch_geometric.utils import to_networkx
from torch_geometric.loader import DataLoader
import networkx as nx
import matplotlib.pyplot as plt

# Checks if GPU is available, else use CPU

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

# Loading the Protein Dataset

from torch_geometric.datasets import TUDataset

dataset = TUDataset(root='.', name='PROTEINS').shuffle()

# Print information about the dataset
print(f'Dataset: {dataset}')
print('-------------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')
print('-------------------')
# Nodes of nodes of a random graph
print(f'Number of nodes: {dataset[0].x.shape[0]}')

# Visualising one of the proteins

G = to_networkx(dataset[2], to_undirected=True)

# 3D spring layout
pos = nx.spring_layout(G, dim=3, seed=0)

# Extract node and edge positions from the layout
node_xyz = np.array([pos[v] for v in sorted(G)])
edge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])

# Create the 3D figure
fig = plt.figure(figsize=(16,16))
ax = fig.add_subplot(111, projection="3d")

# Suppress tick labels
for dim in (ax.xaxis, ax.yaxis, ax.zaxis):
    dim.set_ticks([])

# Plot the nodes - alpha is scaled by "depth" automatically
ax.scatter(*node_xyz.T, s=500, c="#0A047A")

# Plot the edges
for vizedge in edge_xyz:
    ax.plot(*vizedge.T, color="tab:gray")

# fig.tight_layout()
plt.show()

"""Above, we can see a 3D representation of one of the protein graphs. The purple circles are the nodes (amino acids) and the lines representing the edges (connections) between two nodes."""

''' We will be splitting the dataset into training, validation, and test sets.
    This allows us prevent overfitting of the model, obtain the best model, evaluate the model,
    and many more.
'''

train_dataset = dataset[:int(len(dataset)*0.8)]
val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]
test_dataset  = dataset[int(len(dataset)*0.9):]

''' Next, we define our data loaders. These dataloaders let us comfortably load data
    into our models. Eases data loading and saves computational overload and cost.
'''

train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True
    )
val_loader = DataLoader(
    val_dataset,
    batch_size=64,
    shuffle=True
    )
test_loader = DataLoader(
    test_dataset,
    batch_size=64,
    shuffle=False
    )

# What the information in these dataloaders look like

p=0
for i in train_loader:
    print(i)
    p = p+1

    if p > 2:
        break

# Defining loss function and readout function

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self):
        super(LabelSmoothingCrossEntropy, self).__init__()
    def forward(self, x, target, smoothing=0.1):
        confidence = 1. - smoothing
        logprobs = F.log_softmax(x, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + smoothing * smooth_loss
        return loss.mean()


def graph_readout(x, method, batch):
    if method == 'mean':
        return global_mean_pool(x,batch)

    elif method == 'meanmax':
        x_mean = global_mean_pool(x,batch)
        x_max = global_max_pool(x,batch)
        return torch.cat((x_mean, x_max), dim=1)

    elif method == 'sum':
        return global_add_pool(x,batch)

    else:
        raise ValueError('Undefined readout opertaion')

"""Above, we defined the loss function that will be used during the training, and the graph readout function.

Because GNNs usually learn information on the nodes level of a graph, to perform a graph-level classification (i.e. the entire protein in this context instead of the amino acids), we have to aggregate the information on the node-level to the graph-level. This is what the graph readout function does.
"""

# Defining GNN models architectures

'''
    Below we define the architectures of each of the models we mentioned earlier.
    Each of the models implements the graph readout function from above to
    aggregate information to the graph-level.
'''

class Abstract_GNN(torch.nn.Module):
    """
    An Abstract class for all GNN models
    Subclasses should implement the following:
    - forward()
    - predict()
    """
    def __init__(self, num_nodes, f1, f2, readout):
        super(Abstract_GNN, self).__init__()
        self.readout = readout

    def _reset_parameters(self):
            for p in self.parameters():
                if p.dim() > 1:
                    nn.init.xavier_uniform_(p)
                else:
                    nn.init.uniform_(p)

    def forward(self, data, edge_index, batch):

        raise NotImplementedError


class GCN(Abstract_GNN):
    def __init__(self, num_nodes, f1, f2, readout, **kwargs):
        super().__init__(num_nodes, f1, f2, readout)
        self.readout = readout
        self.conv1 = GCNConv(num_nodes, f1)
        self.conv2 = GCNConv(f1, f2)

        last_dim = 2 if readout=='meanmax' else 1
        self.mlp = nn.Linear(f2*last_dim, f2)
        self._reset_parameters()


    def forward(self, data, edge_index, batch):
        x = data
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        x = graph_readout(x, self.readout, batch)
        x = self.mlp(x)
        return x



class GIN(Abstract_GNN):
    def __init__(self, num_nodes, f1, f2, readout, **kwargs):
        super().__init__(num_nodes, f1, f2, readout)
        self.conv1 = GINConv(
            Sequential(Linear(num_nodes, f1), BatchNorm1d(f1), ReLU(),
                       Linear(f1, f1), ReLU()))

        self.conv2 = GINConv(
            Sequential(Linear(f1, f1), BatchNorm1d(f1), ReLU(),
                       Linear(f1, f1), ReLU()))

        self.conv3 = GINConv(
            Sequential(Linear(f1, f2), BatchNorm1d(f2), ReLU(),
                       Linear(f2, f2), ReLU()))

        last_dim = 2 if readout=='meanmax' else 1

        self.last = Linear(f2*last_dim, f2)

        self._reset_parameters()

    def forward(self, data, edge_index, batch):
        x = data
        x = self.conv1(x, edge_index)
        x = self.conv2(x, edge_index)
        x = self.conv3(x, edge_index)
        x = graph_readout(x, self.readout, batch)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.last(x)
        return x


class GAT(Abstract_GNN):
    def __init__(self, num_nodes, f1, f2, readout, concat, num_heads, **kwargs):
        super().__init__(num_nodes, f1, f2, readout)

        self.conv1 = GATv2Conv(num_nodes, f1, heads=num_heads, concat=concat)
        m = num_heads if concat else 1
        self.conv2 = GATv2Conv(f1*m, f2, heads=1)
        last_dim = 2 if readout=='meanmax' else 1
        self.mlp = nn.Linear(f2*last_dim, f2)
        self._reset_parameters()


    def forward(self, data, edge_index, batch):
        x = data
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        x = graph_readout(x, self.readout, batch)
        x = self.mlp(x)
        return x

class ChebC(Abstract_GNN):
    def __init__(self, num_nodes, f1, f2, k, readout):
        super().__init__(num_nodes, f1, f2, readout)
        self.conv1 = ChebConv(num_nodes, f1, k)
        self.conv2 = ChebConv(f1, f1, k)
        self.conv3 = ChebConv(f1, f1, k)
        self.readout = readout
        last_dim = 2 if readout == 'meanmax' else 1
        self.fc = nn.Linear(f1 * last_dim, f2)

    def forward(self, data, edge_index, batch):
        x = data
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = graph_readout(x, self.readout, batch)
        x = self.fc(x)
        return x

class ChebEdge(Abstract_GNN):
    def __init__(self, num_nodes, f1, f2, k, readout):
        super().__init__(num_nodes, f1, f2, readout)
        self.conv1 = ChebConv(num_nodes, f1, k)
        self.conv2 = ChebConv(f1, f1, k)
        self.conv3 = ChebConv(f1, f1, k)

        self.edgeconv1 = EdgeConv(nn.Sequential(nn.Linear(f1*2, f1*2), nn.ReLU(), nn.Linear(f1*2, f1)))
        self.edgeconv2 = EdgeConv(nn.Sequential(nn.Linear(f1*2, f1*2), nn.ReLU(), nn.Linear(f1*2, f1)))

        last_dim = 2 if readout == 'meanmax' else 1
        self.fc = nn.Linear(f1 * last_dim, f2)

    def forward(self, data, edge_index, batch):
        x = data
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.edgeconv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.edgeconv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = graph_readout(x, self.readout, batch)
        x = self.fc(x)
        return x

# Defining training, validation, and test functions

def train(model, train_loader, val_loader, device):

    model = model.to(device)

    loss_function = LabelSmoothingCrossEntropy()
    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-5)
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 400, eta_min = 1e-5)

    # start a typical PyTorch training
    best_metric = -1
    best_metric_epoch = -1
    best_val_loss = 1000
    best_model = None
    epochs = 500

    print('-'*30)
    print ('Training ... ')
    early_stop = 30
    es_counter = 0

    for epoch in range(epochs):

        print("-" * 10)
        print(f"epoch {epoch + 1}/{epochs}")
        model.train()
        epoch_train_loss = 0

        for i, data in enumerate(tqdm(train_loader)):
            batch = data.batch.to(device)
            x = data.x.to(device)
            y = data.y.to(device)
            z = data.edge_index.to(device)
            optimizer.zero_grad()

            out = model(x,z,batch)

            step_loss = loss_function(out, y)
            step_loss.backward(retain_graph=True)
            optimizer.step()
            epoch_train_loss += step_loss.item()

        epoch_train_loss = epoch_train_loss/(i+1)
        lr_scheduler.step()
        val_loss, val_acc = validate_model(model, val_loader, device)
        print(f"epoch {epoch + 1} train loss: {epoch_train_loss:.4f}")

        if val_loss < best_val_loss:
            best_metric = val_acc
            best_val_loss = val_loss
            best_metric_epoch = epoch + 1
            best_model = deepcopy(model)
            print("saved new best metric model")
            es_counter = 0
        else:
            es_counter += 1

        if es_counter > early_stop:
            print('No loss improvment.')
            break

        # wandb_metric = {'train_loss':epoch_train_loss, 'val_loss':val_loss, 'val_acc': val_acc, 'best_val_loss': best_val_loss }
        # wandb.log(wandb_metric)

        print(
            "current epoch: {} current val loss {:.4f} current accuracy: {:.4f}  best accuracy: {:.4f} at loss {:.4f} at epoch {}".format(
                epoch + 1, val_loss, val_acc, best_metric, best_val_loss, best_metric_epoch))


    print(f"train completed, best_val_loss: {best_val_loss:.4f} at epoch: {best_metric_epoch}")

    return best_model


def validate_model(model, val_loader, device):
    model.eval()
    val_loss = 0
    loss_func = nn.CrossEntropyLoss()

    labels = []
    preds = []
    for i, data in enumerate(val_loader):
            batch = data.batch.to(device)
            x = data.x.to(device)
            label = data.y.to(device)
            z = data.edge_index.to(device)

            out = model(x,z,batch)

            step_loss = loss_func(out, label)
            val_loss += step_loss.detach().item()
            preds.append(out.argmax(dim=1).detach().cpu().numpy())
            labels.append(label.cpu().numpy())
    preds = np.concatenate(preds).ravel()
    labels =  np.concatenate(labels).ravel()
    acc = balanced_accuracy_score(preds, labels)
    loss = val_loss/(i+1)

    return loss, acc

def test_model(model, test_loader, device):
    model.eval()
    labels = []
    preds = []
    for i, data in enumerate(test_loader):
            batch = data.batch.to(device)
            x = data.x.to(device)
            label = data.y.to(device)
            z = data.edge_index.to(device)

            out = model(x,z,batch)
            preds.append(out.argmax(dim=1).detach().cpu().numpy())
            labels.append(label.cpu().numpy())
    preds = np.concatenate(preds).ravel()
    labels =  np.concatenate(labels).ravel()

    accuracy = balanced_accuracy_score(labels, preds)
    precision = precision_score(labels, preds)
    recall = recall_score(labels, preds)
    f1 = f1_score(labels, preds)

    return accuracy, precision, recall, f1

"""Above, we defined the training, validation, and test functions.

We will call the training function whenever we need to train any of the 5 models. The process is iterative, training and validating the model at the same time. When the training is done, the function will return the best version of the current model (i.e. the point at which the model achieved optimal training metrics).

The test function allows us test each trained model with the the test dataset, a dataset the model has not been exposed to before. We can then evaluate the model's performance on new unseen data.
"""

# # Initialise logging to store training metrics

# wandb.init(project="gb")

"""# Training the Models

Below, we will initialise each of the 5 models and train them
"""

# Initialise and train GCN Model

GCN_ = GCN(dataset.num_node_features, 32, dataset.num_classes, readout='meanmax')
gcnModel = train(GCN_, train_loader, val_loader, device)

# Initialise and train GIN Model

GIN_ = GIN(dataset.num_node_features, 32, dataset.num_classes, readout='mean')
ginModel = train(GIN_, train_loader, val_loader, device)

# Initialise and train GAT Model

GAT_ = GAT(dataset.num_node_features, 32, dataset.num_classes, readout='sum', concat=True, num_heads=8)
gatModel = train(GAT_, train_loader, val_loader, device)

# Initialise and train Chebnet Model

chebFilterSize = 16

ChebC_ = ChebC(dataset.num_node_features, 32, dataset.num_classes, chebFilterSize, readout='meanmax')
chebModel = train(ChebC_, train_loader, val_loader, device)

# Initialise and train Chebnet + EdgeConv Model

chebFilterSize = 16

ChebEdge_ = ChebEdge(dataset.num_node_features, 32, dataset.num_classes, chebFilterSize, readout='meanmax')
chbedgModel = train(ChebEdge_, train_loader, val_loader, device)

"""# Evaluating Models Perfomance

The training processes are completed. Next up, we compare the performance of the models.
"""

# Evaluate each model on the test dataset and store the results
gcn_accuracy, gcn_precision, gcn_recall, gcn_f1 = test_model(gcnModel, test_loader, device)
gin_accuracy, gin_precision, gin_recall, gin_f1 = test_model(ginModel, test_loader, device)
gat_accuracy, gat_precision, gat_recall, gat_f1 = test_model(gatModel, test_loader, device)
cheb_accuracy, cheb_precision, cheb_recall, cheb_f1 = test_model(chebModel, test_loader, device)
cheb_edge_accuracy, cheb_edge_precision, cheb_edge_recall, cheb_edge_f1 = test_model(chbedgModel, test_loader, device)

# Create a DataFrame to store the results
data = {
    "Model Name": ["GCN", "GIN", "GAT", "Chebnet", "Chebnet + EdgeConv"],
    "Balanced Accuracy": [gcn_accuracy, gin_accuracy, gat_accuracy, cheb_accuracy, cheb_edge_accuracy],
    "Precision": [gcn_precision, gin_precision, gat_precision, cheb_precision, cheb_edge_precision],
    "Recall": [gcn_recall, gin_recall, gat_recall, cheb_recall, cheb_edge_recall],
    "F1-score": [gcn_f1, gin_f1, gat_f1, cheb_f1, cheb_edge_f1]
}

df = pd.DataFrame(data)

print(df)